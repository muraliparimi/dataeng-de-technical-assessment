{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineer - Technical Assessment\n",
    "\n",
    "In this section of the interview at Beyond Finance, you will be assessed on your ability to perform several Data Engineering tasks. To perform well on this task, you will demonstate competence in the following areas:\n",
    "\n",
    "* preprocessing data to prepare for a database load\n",
    "* understanding entity relationships in a database\n",
    "* merging data from different tables\n",
    "* filtering data to relevant subsets\n",
    "* calculating aggregations and descriptive statistics\n",
    "\n",
    "It will be pretty difficult to complete all questions in the allotted time. Your goal is not to speed through the answers, but to come up with answers that demonstrate your knowledge. It's more about your thought process and logic than getting the right answer or your code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "This exercise will be broken into 2 parts\n",
    "1. Data Processing\n",
    "2. Data Wrangling\n",
    "\n",
    "### Data Processing\n",
    "In this section you will take files from the ./raw_data/ subfolders, combine them into a single newline-delimited `json.gz` file per subfolder, and place that CSV file in a ./processed_data/ directory. You may have to do some light investigation into the data files to understand their file formats and delimiters\n",
    "\n",
    "**Example**\n",
    "\n",
    "Files\n",
    "- ./raw_data/tracks/tracks_0.csv\n",
    "- ./raw_data/tracks/tracks_1.json\n",
    "- ./raw_data/tracks/tracks_2.csv\n",
    "- etc... \n",
    "\n",
    "should be combined into a single file ./processed_data/tracks.json.gz\n",
    "\n",
    "**What we look for**\n",
    "\n",
    "- Can you handle all subfolders in a single pass over the raw data files? \n",
    "  - [Murali Parimi] My process handles all subfolders in one single pass one by one\n",
    "- What if the file sizes are in GigaBytes? Can your code (if run on a standard laptop) load the files without going out of memory? (hint `chunksize`)\n",
    "  - [Murali Parimi] I am using generators and it loads one row at a time to memory. I have provided a pandas version as well in this assignment in a separate cell\n",
    "     - The pure python generator version may fail if the json files come in as an array of json objects\n",
    "- Can you identify edge cases? What scenarios could break your code?\n",
    "  - [Murali Parimi]\n",
    "    - deduplication logic not in place, may bloat volume\n",
    "    - quality checks to ensure dollar amounts makes sense, names are really names, dates are dates etc. -- this will not break code though\n",
    "    - if a different format files or compressed files are landed, we may see failures or at least being logged\n",
    "    - if we have a very very large lines that couldnt fit in the memory even though we are reading one line at a time.\n",
    "\n",
    "- Please directly respond to the above questions in your submission.\n",
    "\n",
    "### Data Wrangling\n",
    "For this section, we'll pretend you loaded the raw data plus additional tables into a small SQLite database containing roughly a dozen tables. **We've provided this database for you so don't worry about loading it yourself**. If you are not familiar with the SQLite database, it uses a fairly complete and standard SQL syntax, though does not many advanced analytics functions. Consider it just a remote datastore for storing and retrieving data from. \n",
    "\n",
    "![](db-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "!pip install memory_profiler\n",
    "!pip install ijson\n",
    "%pip install ipython-sql\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89ed2fb",
   "metadata": {},
   "source": [
    "### Python File Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import csv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "LANDING_ZONE = Path(\"./raw_data\")\n",
    "PROCESSED_ZONE = Path(\"./processed_data\")\n",
    "PROCESSED_ZONE.mkdir(exist_ok=True)\n",
    "LOG_ZONE = Path(\"./logs\")\n",
    "\n",
    "LOG_FILE = LOG_ZONE / f\"unsupported_files_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "\n",
    "def read_pd_csv(file_path):\n",
    "    for chunk in pd.read_csv(file_path, chunksize=10000):\n",
    "        yield from chunk.to_dict(orient=\"records\")\n",
    "\n",
    "def read_pd_json(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            first_char = f.read(1)\n",
    "            f.seek(0)\n",
    "            if first_char == \"[\":\n",
    "                data = json.load(f)\n",
    "                for row in data:\n",
    "                    yield row\n",
    "            else:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        try:\n",
    "                            yield json.loads(line)\n",
    "                        except json.JSONDecodeError:\n",
    "                            continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "def pd_process_folder(subfolder_path):\n",
    "    output_path = PROCESSED_ZONE / f\"{subfolder_path.name}.json.gz\"\n",
    "    with gzip.open(output_path, \"wt\", encoding=\"utf-8\") as out_file:\n",
    "        for file in subfolder_path.iterdir():\n",
    "            if not file.is_file():\n",
    "                continue\n",
    "\n",
    "            ext = file.suffix.lower()\n",
    "            if ext == \".csv\":\n",
    "                for record in read_pd_csv(file):\n",
    "                    json.dump(record, out_file)\n",
    "                    out_file.write(\"\\n\")\n",
    "            elif ext == \".json\":\n",
    "                for record in read_pd_json(file):\n",
    "                    json.dump(record, out_file)\n",
    "                    out_file.write(\"\\n\")\n",
    "            else:\n",
    "                print(f\"Unsupported file format: {file}\")\n",
    "\n",
    "def read_csv(file_path):\n",
    "    with open(file_path, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            yield row\n",
    "\n",
    "def read_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        first_char = f.read(1)\n",
    "        f.seek(0)\n",
    "        if first_char == '[':\n",
    "            # Entire list-style JSON\n",
    "            data = json.load(f)\n",
    "            for item in data:\n",
    "                yield item\n",
    "        else:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        yield json.loads(line)\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "\n",
    "def read_with_ijson(file_path):\n",
    "    \"\"\"\n",
    "    reads records from a JSON file contained in array using ijson.\n",
    "    \"\"\"\n",
    "    import ijson\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for record in ijson.items(f, 'item'):\n",
    "            yield record\n",
    "\n",
    "def read_records(file_path):\n",
    "    ext = file_path.suffix.lower()\n",
    "    if ext == '.csv':\n",
    "        yield from read_csv(file_path)\n",
    "    elif ext == '.json':\n",
    "        yield from read_json(file_path)\n",
    "    else:\n",
    "        with open(LOG_FILE, \"a\") as log_file:\n",
    "            log_file.write(f\"[SKIPPED] unsupported file format: {file_path}\")\n",
    "\n",
    "\n",
    "def process_folder(subfolder):\n",
    "    output_path = PROCESSED_ZONE / f\"{subfolder.name}.json.gz\"\n",
    "    with gzip.open(output_path, 'wt', encoding='utf-8') as out_file:\n",
    "        for file in subfolder.iterdir():\n",
    "            if file.is_file() and file.suffix.lower() in {'.csv', '.json'}:\n",
    "                for record in read_records(file):\n",
    "                    json.dump(record, out_file)\n",
    "                    out_file.write('\\n')\n",
    "\n",
    "def main(option):\n",
    "    for folder in LANDING_ZONE.iterdir():\n",
    "        if folder.is_dir():\n",
    "            if option == 'pandas':\n",
    "                print(f\"processing with pandas: {folder.name}\")\n",
    "                process_folder(folder)\n",
    "            else:\n",
    "                print(f\"processing in python generators : {folder.name}\")\n",
    "                pd_process_folder(folder)                \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%memit\n",
    "main(\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install prettytable==2.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prettytable\n",
    "prettytable.DEFAULT_STYLE = prettytable.PLAIN_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql \n",
    "%sql sqlite:///db/sqlite/chinook.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "con = sqlite3.connect(\"db/sqlite/chinook.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How many different customers are there? \n",
    "**Answer** \n",
    "- 59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select count(distinct customerid) from customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How long is the longest track in minutes?\n",
    "\n",
    "**Answer**\n",
    "- 88 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "select round(max(Milliseconds) / (1000 * 60)) from tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Which genre has the shortest average track length?\n",
    "\n",
    "**Answer**\n",
    "- Sci Fi & Fantasy has 49 minutes of average playtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "select g.name, round(avg(milliseconds)/ (1000*60)) avg_length from tracks t join genres g on t.genreid = g.genreid\n",
    "group by g.name \n",
    "order by avg_length desc\n",
    "limit 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Which artist shows up in the most playlists?\n",
    "\n",
    "**Answer**\n",
    "- \"None/NA\" appears to be the most with their tracks appearing in 12 playlists.\n",
    "- Additional information:  \"None\" or \"NA\" shows as most played (2262 tracks) followed by \"Steve Harris\" with 193 tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "select composer, count(distinct playlistid) as num_playlists from playlist_track p join tracks t on p.trackid = t.trackid\n",
    "group by composer\n",
    "order by num_playlists desc\n",
    "limit 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. What album had the most purchases?\n",
    "\n",
    "**Answer**\n",
    "\n",
    "Categories\n",
    "-  most purchases by dollar amount : \"Greatest Hits\" with $232.81\n",
    "-  most purchases by number of invoices:  \"Greatest Hits\" with 16 different invoices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "\n",
    "with title_invoice as (\n",
    "    select a.title, i.invoiceid from invoice_items i join tracks t on i.trackid = t.trackid join albums a on t.albumid = a.albumid \n",
    "    group by a.title, i.invoiceid\n",
    ")\n",
    "select title, sum(total) as total_sales from title_invoice ti join invoices i on ti.invoiceid = i.invoiceid\n",
    "group by title\n",
    "order by total_sales desc\n",
    "limit 1\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Which customer has the highest number of sales in terms of dollars?\n",
    "\n",
    "**Answer**\n",
    "- Helena HolÃ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select c.Firstname || \" \" || c.Lastname as name from invoices I join customers c on i.customerid = c.customerid group by name order by sum(total) desc limit 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Count of customers who have dollar sales more than $40?\n",
    "\n",
    "**Answer**\n",
    "\n",
    "- 14 customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select count(*) from (select customerid, sum(total) total_dollars from invoices group by customerid having total_dollars > 40) a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
